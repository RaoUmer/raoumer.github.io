<!DOCTYPE html>
<html>
  <head>
    <title>Open Data Resources For Data Science Research: Part-2 </title>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1"
<link rel="apple-touch-icon" sizes="57x57" href="../assets/img/apple-touch-icon-57x57.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="60x60" href="../assets/img/apple-touch-icon-60x60.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="72x72" href="../assets/img/apple-touch-icon-72x72.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="76x76" href="../assets/img/apple-touch-icon-76x76.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="114x114" href="../assets/img/apple-touch-icon-114x114.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="120x120" href="../assets/img/apple-touch-icon-120x120.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="144x144" href="../assets/img/apple-touch-icon-144x144.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="152x152" href="../assets/img/apple-touch-icon-152x152.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="180x180" href="../assets/img/apple-touch-icon-180x180.png?v=wAAv6Wqe6l">
<link rel="manifest" href="../assets/img/manifest.json?v=wAAv6Wqe6l">
<link rel="shortcut icon" href="../assets/img/favicon.ico?v=wAAv6Wqe6l">
<meta name="msapplication-TileColor" content="#e74c3c">
<meta name="msapplication-TileImage" content="/assets/img/mstile-144x144.png?v=wAAv6Wqe6l">
<meta name="msapplication-config" content="/assets/img/browserconfig.xml?v=wAAv6Wqe6l">
<meta name="theme-color" content="#e74c3c">
    <link rel="stylesheet" type="text/css" href="../assets/css/uno-zen.css?v=d88d979280" />
    <link rel="canonical" href="http://raoumer.com/_posts/data_sources_part_2.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="Rao Muhammad Umer" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Open Data Resources For Data Science Research: Part-2" />
    <meta property="og:description" content="This blog post is for introducing Open Data Resources for Data Science research." />
    <meta property="og:url" content="u=http://raoumer.com/blog_posts/data_sources.html" />
    <meta property="og:image" content="u=http://raoumer.com/images" />
    <meta property="article:published_time" content="2016-08-24T14:20:00.000Z" />
    <meta property="article:modified_time" content="2016-08-24T08:34:04.543Z" />
    <meta property="article:tag" content="webapp" />
    <meta property="article:tag" content="webapp" />
    <meta property="article:tag" content="webapp" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="" />
    <meta name="twitter:description" content="This blog post is for introducing Data Resources for Open Data Resources for Data Science research." />
    <meta name="twitter:url" content="u=http://raoumer.com/blog_posts/data_sources_part_2.html" />
    <meta name="twitter:image:src" content="u=http://raoumer.com/images" />
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "Rao Muhammad Umer",
    "author": {
        "@type": "Person",
        "name": "Rao Muhammad Umer",
        "url": "u=http://raoumer.com/",
        "sameAs": null,
        "description": null
    },
    "headline": "Open Data Resources For Data Science Research",
    "url": "u=http://raoumer.com/blog_posts/data_sources_part_2.html",
    "datePublished": "2017-07-31T14:20:00.000Z",
    "dateModified": "2017-07-31T08:34:04.543Z",
    "image": "u=http://raoumer.com/",
    "keywords": "data sources for data science research: Part-2",
    "description": "This blog post is for introducing Open Data Resources for Data Science Research."
}
    </script>
    <script>
var open_button = '.nav-blog > a'
</script>
<script>
var profile_title = 'Rao Muhammad Umer';
</script>
<script>
var disqus_shortname = 'raomumer';
</script>
<script>
var profile_resume ='Graduate Student';
</script>
  </head>
  <body class="post-template tag-datasources">
    <header id="menu-button" class="expanded">
      <a><i class="icon icon-list"></i></a>
    </header>
    <aside class="cover" style="background: url(../images/cover_6.png) center/cover no-repeat fixed">
  <div class="cover container">
    <div class="profile">
      <a id="avatar-link" title="link to homepage for Rao Muhammad Umer" href="http://raoumer.com/#open">
        <img src="../images/raoumer.jpg" alt="Rao Umer avatar" class="profile avatar rounded hvr-buzz-out" />
        <h1 id="profile-title">Rao Muhammad Umer</h1>
        <h3 id="profile-resume"></h3>
      </a>

      <hr class="divider long" />
      <p>I&#x27;m a Lecturer in department of Computer Science and IT at The University of Lahore. I blog about Machine Learning, Deep Learning, and Data Science.</p>
      <hr class="divider short" />
      <div class="navigation">
        <div class="profile contact">
          <nav class="navigation left">
  <ul class="links">
      <li class="nav-blog ">
        <a href="http://raoumer.com/#open">Blog</a>
      </li>
      <li class="nav-about nav-current ">
        <a href="http://raoumer.com/about.html">About</a>
      </li>
	<li class="nav-courses nav-current">
	<a href="http://raoumer.com/courses.html">Teaching</a>
      </li>  
      <li class="nav-portfolio nav-current ">
        <a href="http://raoumer.com/portfolio.html">Research</a>
      </li>
      <li class="nav-contact nav-current ">
        <a href="http://raoumer.com/contact.html">Contact</a>
      </li>
  </ul>
</nav>

          
<nav class="navigation right">
  <ul class="social expanded">

  <!-- Twitter -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="http://twitter.com/roumer_swl" title="@raoumer_swl on Twitter">
      <i class='icon icon-social-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>

  <!-- Linkedin -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://www.linkedin.com/in/raomumer" title="raoumer on LinkedIn">
      <i class='icon icon-social-linkedin'></i>
      <span class="label">Linkedin</span>
    </a>
  </li>

  <!-- Github -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://github.com/RaoUmer" title="raoumer on Github">
      <i class='icon icon-social-github'></i>
      <span class="label">Github</span>
    </a>
  </li>

  <!-- E-mail -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="mailto:engr.raoumer943@gmail.com" title="send me an email">
      <i class='icon icon-mail'></i>
      <span class="label">Email</span>
    </a>
  </li>

  </ul>
</nav>

        </div>
      </div>
    </div>
  </div>
</aside>
    <main>
      <section id="search-results"></section>
      <section class="content">
        

  <article class="post tag-datasets tag-ML tag-DL tag-DS">
    <header>
      <div class="post meta">
        <time datetime="31 July 2017">31 July 2017</time>
        <span class="post tags">in <a href="../blog_posts/data_sources_part_2.html">Open Data Resources For Data Science Research: Part-2</a> </span>


        <span class="post reading-time"> ~ <span></span> read.</span>
      </div>
      <a alt="Tweet 'Open Data Resources For Data Science Research: Part-2'" href="https://twitter.com/intent/tweet?text=Open%20Data%20Resources%20for%20Data%20Science%20Research%20%C2%BB&amp;hashtags=&amp;url=http://raoumer.com/blog_posts/data_sources_part_2.html">
        <img id="post-image" src="../images/ds.jpg" alt="Open Data Resources For Data Science Research: Part-2">
		<h1 class="icon-reverse icon-social-twitter-post" id="post-title">Open Data Resources For Data Science Research: Part-2</h1>
      </a>
    </header>

    <div id="post-content" class="post tag-webapp tag-github tag-flask tag-heroku">

	<p> This blog post is continuing the pervious <a href="http://raoumer.com/blog_posts/data_sources.html">post on open datasets</a>. Here are more cool public open data sources you can use for your projects related to Data Science, Machine Learning and Deep Learning research:</p>
	<ul>
	
	<li> <strong> ImageNet Dataset</strong>
	<ul>
	<li><a href="http://image-net.org/index"> <strong>ImageNet Dataset:</strong></a>  ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. </li>
	</ul>
	</li>
	<li> <strong> Open Data for Deep Learning</strong>
	<ul>
	<li><a href="https://deeplearning4j.org/opendata"> <strong>Open Data for Deep Learning:</strong></a>  Here you’ll find an organized list of interesting, high-quality datasets for machine learning and deep learning research. </li>
	</ul>
	</li>
	<li> <strong> List of datasets for machine learning research</strong>
	<ul>
	<li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research"> <strong>List of datasets for machine learning research:</strong></a>  These datasets are used for machine learning research and have been cited in peer-reviewed academic journals and other publications. </li>
	</ul>
	</li>
	<li> <strong> NEXET Dataset</strong>
	<ul>
	<li><a href="https://www.getnexar.com/challenge-2/"> <strong>NEXET Dataset:</strong></a> The Nexar dataset is a massive set consisting of 50,000 images from all over the world with bounding box annotations of the rear of vehicles collected from a variety of locations, lighting, and weather conditions. We are releasing this dataset to you, our challengers, to empower you to build a truly smart collision prevention system that can work extremely well anywhere and at any time. </li>
	</ul>
	</li>
	<li> <strong> Kinect Gesture Data Set</strong>
	<ul>
	<li><a href="http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/"> <strong>Kinect Gesture Data Set:</strong></a> The Microsoft Research Cambridge-12 Kinect gesture data set consists of sequences of human movements, represented as body-part locations, and the associated gesture to be recognized by the system. </li>
	</ul>
	</li>
	<li> <strong> MSRA-CFW: Data Set of Celebrity Faces on the Web</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/research/project/msra-cfw-data-set-of-celebrity-faces-on-the-web/"> <strong>Data Set of Celebrity Faces on the Web:</strong></a> The dataset includes image URLs for 202792 faces. The labels of the faces are automatically generated by the algorithm, with high accuracy. To facilitate downloading the images, we provide a number of URLs for the near-duplicates of each face. Besides, the thumbnail images and facial features(LBP) are also provided for visualization and benchmarking purposes. </li>
	</ul>
	</li>	
	<li> <strong> Image Cropping Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52457"> <strong>Image Cropping Dataset:</strong></a> The Image Cropping Dataset contains the cropping parameters for 1000 images that were manually cropped by an experienced photographer. </li>
	</ul>
	</li>
	<li> <strong> Visual Question Generation dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=53670"> <strong>Visual Question Generation dataset:</strong></a> We introduce this dataset in order to support the novel task of Visual Question Generation (VQG), where, given an image, the system should ‘ask a natural and engaging question’. This dataset can be used to support research on common sense reasoning and compute-human conversational systems. </li>
	</ul>
	</li>
	<li> <strong> Smart Selection Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52526"> <strong>Smart Selection Dataset:</strong></a> Smart selection is the task of predicting the span of text that a user intended to select after they touched on a single word on a touch-enabled device.  </li>
	</ul>
	</li>
	<li> <strong> MSR Demosaicing Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52535"> <strong>MSR Demosaicing Dataset:</strong></a> The Microsoft Research Cambridge demosaicing data set consists of set of raw images, and their downscaled versions which can be used for learning and evaluating demosaicing (and possibly other tasks like denoising), both in linear-space and color-space. </li>
	</ul>
	</li>
	<li> <strong> Abstract Scene Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52035"> <strong>Abstract Scene Dataset:</strong></a> This dataset contains clip art related to the academic paper Bringing Semantics Into Focus Using Visual Abstraction. </li>
	</ul>
	</li>
	<li> <strong> FingerPaint Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52288"> <strong>FingerPaint Dataset:</strong></a> The FingerPaint Dataset contains video-sequences of several individuals performing hand gestures, as captured by a depth camera. </li>
	</ul>
	</li>
	<li> <strong> Microsoft Document Aboutness Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52295"> <strong>Microsoft Document Aboutness Dataset:</strong></a> The Microsoft Document Aboutness Dataset consists of randomly sampled URLs (from a HEAD and TAIL distribution), all entities recognized in those documents, and a relevance assessment for each entity/URL pair as to whether or not the entity is salient to the content of the URL.</li>
	</ul>
	</li>
	<li> <strong> MSR Abstractive Text Compression Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54262"> <strong>MSR Abstractive Text Compression Dataset:</strong></a> This dataset contains sentences and short paragraphs with corresponding shorter (compressed) versions. There are up to five compressions for each input text, together with quality judgements of their meaning preservation and grammaticality.</li>
	</ul>
	</li>
	<li> <strong> WebQuestions Semantic Parses Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52763"> <strong>WebQuestions Semantic Parses Dataset:</strong></a> The WebQuestionsSP dataset is released as part of our ACL-2016 paper “The Value of Semantic Parse Labeling for Knowledge Base Question Answering” [Yih, Richardson, Meek, Chang & Suh, 2016], in which we evaluated the value of gathering semantic parses, vs. answers, for a set of questions that originally comes from WebQuestions [Berant et al., 2013].</li>
	</ul>
	</li>
	<li> <strong>Election 2012 Tweet ID dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52598"> <strong>Election 2012 Tweet ID dataset:</strong></a> This data set identifies 38M tweets collected for the analysis of social media messages related to the 2012 U.S.</li>
	</ul>
	</li>
	<li> <strong> MSR 3D Video Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52358"> <strong>MSR 3D Video Dataset:</strong></a> This data includes a sequence of 100 images captured from 8 cameras showing the breakdancing and ballet scenes from the paper “High-quality video view interpolation using a layered representation”, Zitnick et al., SIGGRAPH 2004.</li>
	</ul>
	</li>
	<li> <strong> MSR GPS Privacy Dataset 2009</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54965"> <strong>MSR GPS Privacy Dataset 2009:</strong></a> The table below contains pointers to text files with GPS data taken in the region of Seattle, Washington USA. Each file contains data from one of 21 volunteers who carried a GPS logger with them for approximately eight weeks in the fall of 2009. This set of 21 volunteers is a subset of 37 people who participated in the survey.</li>
	</ul>
	</li>
	<li> <strong>FB15K-237 Knowledge Base Completion Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52312"> <strong>FB15K-237 Knowledge Base Completion Dataset:</strong></a> This dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs, as used in the work published in (Toutanova and Chen CVSM-2015) and (Toutanova et al).</li>
	</ul>
	</li>
	<li> <strong> Avatar Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52026"> <strong>Avatar Dataset:</strong></a> MRS introduce a new corpus of descriptions of Xbox avatars created by actual gamers. </li>
	</ul>
	</li>
	<li> <strong>Diverse Algebra Word Problem Dataset with Derivation Annotations</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52628"> <strong>Diverse Algebra Word Problem Dataset with Derivation Annotations:</strong></a> This dataset provides training and testing examples for solving algebra word problems automatically.</li>
	</ul>
	</li>
	<li> <strong>NCI-PID-PubMed Genomics Knowledge Base Completion Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54054"> <strong>NCI-PID-PubMed Genomics Knowledge Base Completion Dataset:</strong></a> This dataset includes a database of regulation relationships among genes and corresponding textual mentions of pairs of genes in PubMed article abstracts.</li>
	</ul>
	</li>
	<li> <strong> Longitudinal Tweet ID dataset for a selection of Health, Social, and Business Experiences</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54927"> <strong>Longitudinal Tweet ID dataset for a selection of Health, Social, and Business Experiences:</strong></a> This data set consists of the tweet IDs collected for the propensity-score analysis of longitudinal social media messages posted by people who mention specific health, social and business domains. This data set accompanies the paper, “Distilling the Outcomes of Personal Experiences: A Propensity-scored Analysis of Social Media.</li>
	</ul>
	</li>
	<li> <strong>Dataset for Inferring Missing Entity Type Instances for Knowledge Base Completion</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52589"> <strong>Dataset for Inferring Missing Entity Type Instances for Knowledge Base Completion:</strong></a> This is a dataset that can be used for training and evaluating knowledge base completion approaches for inferring missing entity type instances.</li>
	</ul>
	</li>
	<li> <strong>Tweet Entity Linking Dataset: IE-driven and IR-driven sets</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52443"> <strong>Tweet Entity Linking Dataset: IE-driven and IR-driven sets:</strong></a> In this dataset, we release the labeled data for people to evaluate and compare entity linking systems on tweets.</li>
	</ul>
	</li>
	<li> <strong> Learning from Everyday Analog Pen Use to Improve Digital Ink Experiences Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54592"> <strong>Learning from Everyday Analog Pen Use to Improve Digital Ink Experiences Dataset:</strong></a> This is the data released with the CHI 2017 paper: As We May Ink? Learning from Everyday Analog Pen Use to Improve Digital Ink Experiences. It contains the 493 entries of a diary study with 26 participants on their use of analog pen and the 178 entries of a follow-up diary study with 30 participants on their use of digital pen.</li>
	</ul>
	</li>
	<li> <strong>Optical Data</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54267"> <strong>Optical Data:</strong></a> This dataset includes 14 months of optical data from Microsoft’s wide-area backbone network in North America.</li>
	</ul>
	</li>
	<li> <strong>Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=53878"> <strong>Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems:</strong></a> This is a public release of the dataset corresponding the paper "Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems" that will appear in EMNLP 2016.</li>
	</ul>
	</li>
	<li> <strong>Microsoft Research Sequential Question Answering (SQA) Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253"> <strong>Microsoft Research Sequential Question Answering (SQA) Dataset:</strong></a> The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.</li>
	</ul>
	</li>
	<li> <strong>Microsoft Cognitive Toolkit Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/cognitive-toolkit/"> <strong>Microsoft Cognitive Toolkit Dataset:</strong></a> The Microsoft Cognitive Toolkit empowers you to harness the intelligence within massive datasets through deep learning by providing uncompromised scaling, speed and accuracy with commercial-grade quality and compatibility with the programming languages and algorithms you already use.</li>
	</ul>
	</li>
	<li> <strong>GeoLife GPS Trajectories Dataset</strong>
	<ul>
	<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52367"> <strong>GeoLife GPS Trajectories Dataset:</strong></a> This GPS trajectory dataset was collected in (Microsoft Research Asia) Geolife project by 182 users in a period of over three years (from April 2007 to August 2012).</li>
	</ul>
	</li>
	<li> <strong>Wikipedia Biography Dataset</strong>
	<ul>
	<li><a href="https://github.com/DavidGrangier/wikipedia-biography-dataset"> <strong>Wikipedia Biography Dataset:</strong></a> This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation algorithms.</li>
	</ul>
	</li>
	<li> <strong>SpineWeb Dataset</strong>
	<ul>
	<li><a href="http://spineweb.digitalimaginggroup.ca/spineweb/index.php?n=Main.Datasets"> <strong>SpineWeb Dataset:</strong></a> This dataset are collected for Research on Spine Imaging and Image Analysis.</li>
	</ul>
	</li>
	<li> <strong>Stack Exchange Dataset</strong>
	<ul>
	<li><a href="https://archive.org/details/stackexchange"> <strong>Stack Exchange Dataset:</strong></a> This is an anonymized dump of all user-contributed content on the Stack Exchange network. Each site is formatted as a separate archive consisting of XML files zipped via 7-zip using bzip2 compression. Each site archive includes Posts, Users, Votes, Comments, PostHistory and PostLinks.</li>
	</ul>
	</li>
	<li> <strong>The Twitter Stream Grab Dataset</strong>
	<ul>
	<li><a href="https://archive.org/details/twitterstream"> <strong>The Twitter Stream Grab Dataset:</strong></a> A simple collection of JSON grabbed from the general twitter stream, for the purposes of research, history, testing and memory.</li>
	</ul>
	</li>
	<li> <strong>StarData</strong>
	<ul>
	<li><a href="https://github.com/TorchCraft/StarData"> <strong>StarData Dataset:</strong></a> We release the largest StarCraft: Brood War replay dataset yet, with 65646 games. The full dataset after compression is 365 GB, 1535 million frames, and 496 million player actions. The entire frame data was dumped out at 8 frames per second. We made a big effort to ensure this dataset is clean and has mostly high quality replays. You can access it with TorchCraft in C++, Python, and Lua. The replays are in an AWS S3 bucket at s3://stardata.</li>
	</ul>
	</li>
	<li> <strong>Phrasal Recognition Dataset</strong>
	<ul>
	<li><a href="http://vision.cs.uiuc.edu/phrasal/"> <strong>Phrasal Recognition Dataset:</strong></a> This dataset contains 8 object categories from Pascal VOC that are suitable for studying the interactions between objects.</li>
	</ul>
	</li>
	<li> <strong>UIUC Pascal Sentence Dataset </strong>
	<ul>
	<li><a href="http://vision.cs.uiuc.edu/pascal-sentences/"> <strong>UIUC Pascal Sentence Dataset </strong></a> </li>
	</ul>
	</li>
	<li> <strong>Cross Category Object Recognition Dataset (CORE) </strong>
	<ul>
	<li><a href="http://vision.cs.uiuc.edu/CORE/"> <strong>Cross Category Object Recognition Dataset:</strong></a> The CORE dataset is intended to help learn more detailed models and for exploring cross-category generalization in object recognition.</li>
	</ul>
	</li>
	<li> <strong>Attribute Dataset (aPascal, aYahoo) </strong>
	<ul>
	<li><a href="http://vision.cs.uiuc.edu/attributes/"> <strong>Attribute Dataset (aPascal, aYahoo):</strong></a> There are three components to the dataset:<br>
Annotations: The attribute annotations for the aPascal train and test sets, and aYahoo test set.<br>
aYahoo images: Our images collected from Yahoo <br>
aPascal images: These are the images from the Pascal VOC 2008. </li>
	</ul>
	</li>
	<li> <strong>MIMIC Critial Care Dataset </strong>
	<ul>
	<li><a href="https://mimic.physionet.org/"> <strong>MIMIC Critial Care Dataset:</strong></a> MIMIC is an openly available dataset developed by the MIT Lab for Computational Physiology, comprising deidentified health data associated with ~40,000 critical care patients. It includes demographics, vital signs, laboratory tests, medications, and more.</li>
	</ul>
	</li>
	<li> <strong>Fashion-mnist Dataset </strong>
	<ul>
	<li><a href="https://github.com/zalandoresearch/fashion-mnist"> <strong>Fashion-mnist Dataset:</strong></a> A dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement of the original MNIST dataset for benchmarking machine learning algorithms.</li>
	</ul>
	</li>
	<li> <strong>Rdatasets</strong>
	<ul>
	<li><a href="https://vincentarelbundock.github.io/Rdatasets/datasets.html"> <strong>Rdatasets:</strong></a> Rdatasets is a collection of 1173 datasets that were originally distributed alongside the statistical software environment R and some of its add-on packages. The goal is to make these data more broadly accessible for teaching and statistical software development.</li>
	</ul>
	</li>
	<li> <strong>bAbI Project Datasets</strong>
	<ul>
	<li><a href="https://research.fb.com/downloads/babi/"> <strong>bAbI Project Datasets:</strong></a> The bAbI project of Facebook AI Research contains datasets, which is organized towards the goal of automatic text understanding and reasoning.</li>
	</ul>
	</li>
	<li> <strong>Reddit Datasets</strong>
	<ul>
	<li><a href="https://www.reddit.com/r/datasets/"> <strong>Reddit Datasets:</strong></a> Reddit datasets repository.</li>
	</ul>
	</li>
	<li> <strong>Awesome Data Science repository Datasets</strong>
	<ul>
	<li><a href="https://github.com/bulutyazilim/awesome-datascience#data-sets"> <strong>Awesome Data Science repository Datasets</strong></a></li>
	</ul>
	</li>
	<li> <strong>DiaRetDB1 Datasets</strong>
	<ul>
	<li><a href="http://www2.it.lut.fi/project/imageret/diaretdb1_v2_1/"> <strong>DiaRetDB1 Datasets:</strong></a>The DiaRetDB1 is a public database for evaluating and benchmarking diabetic retinopathy detection algorithms. The database contains digital images of eye fundus and expert annotated ground truth for several well-known diabetic fundus lesions (hard exudates, soft exudates, microaneurysms and hemorrhages). The original images and the raw ground truth are both available.</li>
	</ul>
	</li>
	<li> <strong>SBM-RGBD dataset</strong>
	<ul>
	<li><a href="http://rgbd2017.na.icar.cnr.it/SBM-RGBDdataset.html"> <strong>SBM-RGBD dataset:</strong></a>The SBM-RGBD dataset has been created in order to evaluate and compare scene background modelling methods for moving object detection on RGBD videos. It provides all facilities (data, ground truths, and evaluation scripts) for the SBM-RGBD Challenge.</li>
	</ul>
	</li>
	<li> <strong>Classification datasets</strong>
	<ul>
	<li><a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"> <strong>Classification datasets:</strong></a>Discover the current state of the art in objects classification datasets.</li>
	</ul>
	</li>
	<li> <strong>MIT Places datasets</strong>
	<ul>
	<li><a href="http://places.csail.mit.edu/"> <strong>MIT Places datasets:</strong></a>The Places database contains 205 scene categories and 2,5 millions of images.</li>
	</ul>
	</li>
	<li> <strong>3D IKEA dataset</strong>
	<ul>
	<li><a href="http://ikea.csail.mit.edu/"> <strong>3D IKEA dataset:</strong></a>Dataset contains about 759 images and 219 3D models. All 759 images are annotated using available models (about 90 different models).</li>
	</ul>
	</li>
	<li> <strong>SUN Database</strong>
	<ul>
	<li><a href="http://groups.csail.mit.edu/vision/SUN/"> <strong>SUN Database:</strong></a>Scene UNderstanding Database. A database for scene recognition (900 scene categories) and multiclass object detection (>15000 fully segmented images).</li>
	</ul>
	</li>
	<li> <strong>360-SUN Database</strong>
	<ul>
	<li><a href="http://people.csail.mit.edu/jxiao/SUN360/main.html"> <strong>360-SUN Database:</strong></a>A database of 360 degrees panoramas organized along the SUN categories.</li>
	</ul>
	</li>
	<li> <strong>Out of context objects dataset</strong>
	<ul>
	<li><a href="http://people.csail.mit.edu/myungjin/outOfContext.html"> <strong>Out of context objects dataset:</strong></a>The database contains 218 fully annotated images with at least one object out-of-context. Can you detect the out of context object?</li>
	</ul>
	</li>
	<li> <strong>Tiny Images Dataset</strong>
	<ul>
	<li><a href="http://horatio.cs.nyu.edu/mit/tiny/data/index.html"> <strong>Tiny Images Dataset:</strong></a>Dataset consists of 79,302,017 images, each being a 32x32 color image.</li>
	</ul>
	</li>
	<li> <strong>Indoor Scene Recognition Database</strong>
	<ul>
	<li><a href="http://web.mit.edu/torralba/www/indoor.html"> <strong>Indoor Scene Recognition Database:</strong></a>The database contains 67 Indoor categories, and a total of 15620 images. The number of images varies across categories, but there are at least 100 images per category. All images are in jpg format.</li>
	</ul>
	</li>
	<li> <strong>LabelMe Database</strong>
	<ul>
	<li><a href="http://cvcl.mit.edu/database.htm"> <strong>LabelMe Database:</strong></a>Each database is composed of a few hundred images of scenes belonging to the same semantic category. All of the images are in color, in jpeg format, and are 256 x 256 pixels. The sources of the images vary (from commercial databases, websites, digital cameras).</li>
	</ul>
	</li>
	<li> <strong>8 Scene Categories Dataset</strong>
	<ul>
	<li><a href="http://people.csail.mit.edu/torralba/code/spatialenvelope/"> <strong>8 Scene Categories Dataset:</strong></a>This dataset contains 8 outdoor scene categories: coast, mountain, forest, open country, street, inside city, tall buildings and highways.</li>
	</ul>
	</li>
	<li> <strong>GazeFollow Dataset</strong>
	<ul>
	<li><a href="http://gazefollow.csail.mit.edu/download.html"> <strong>GazeFollow Dataset</strong></a></li>
	</ul>
	</li>
	<li> <strong>Full-sized images and segmentations Dataset</strong>
	<ul>
	<li><a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/"> <strong>Full-sized images and segmentations Dataset:</strong></a>All images are fully annotated with objects and, many of the images have parts too.</li>
	</ul>
	</li>
	<li> <strong>MovieBook Dataset</strong>
	<ul>
	<li><a href="http://yknzhu.wixsite.com/mbweb"> <strong>MovieBook Dataset:</strong></a> Ground-truth alignments for 11 movie/book pairs, with shot, subtitle and book data.</li>
	</ul>
	</li>
	<li> <strong>Medical Data for Machine Learning</strong>
	<ul>
	<li><a href="https://github.com/beamandrew/medical-data"> <strong>Medical Datasets:</strong></a> This is a curated list of medical data for machine learning.</li>
	</ul>
	</li>
	<li> <strong>Land Cover Datasets</strong>
	<ul>
	<li><a href="https://worldwidescience.org/topicpages/l/land+cover+datasets.html"> <strong>Land Cover Datasets:</strong></a> This is a curated list of land covers datasets.</li>
	</ul>
	</li>
	<li> <strong>EuroSAT: A Novel Dataset </strong>
	<ul>
	<li><a href="https://arxiv.org/abs/1709.00029"> <strong>EuroSAT Dataset:</strong></a>  A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification.</li>
	</ul>
	</li>
	<li> <strong>Data Set Repositories </strong>
	<ul>
	<li><a href="http://www.datasciencecentral.com/profiles/blogs/a-plethora-of-data-set-repositories"> <strong>Data Set Repositories</strong></a>  These 19 'sets of data sets' cover free or public data from various industries, including small and large, structured and unstructured data sets.</li>
	</ul>
	</li>
	<li> <strong>DSC Datasets</strong>
	<ul>
	<li><a href="http://www.datasciencecentral.com/page/search?q=data+sets"> <strong>DSC Datasets links</strong></a></li>
	</ul>
	</li>
	<li> <strong>Sebastian Raschka Datasets list</strong>
	<ul>
	<li><a href="https://github.com/rasbt/pattern_classification/blob/master/resources/dataset_collections.md"> <strong>Sebastian Raschka Datasets list:</strong></a>A collection of links to various free and open-source datasets.</li>
	</ul>
	</li>
	<li> <strong>Dataset of 230,000 3D facial landmarks</strong>
	<ul>
	<li><a href="https://www.adrianbulat.com/face-alignment"> <strong>Facial landmarks Dataset:</strong></a>LS3D-W is a large-scale 3D face alignment dataset constructed by annotating the images from AFLW, 300VW, 300W and FDDB in a consistent manner with 68 points using the automatic method.</li>
	</ul>
	</li>
	<li> <strong>Deep learning Publicly available Datasets</strong>
	<ul>
	<li><a href="http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/PublicDatasets"> <strong>Deep learning Publicly available Datasets:</strong></a>A collection of datasets collected by LISA lab.</li>
	</ul>
	</li>
	<li> <strong>MILA PUBLIC DATASETS</strong>
	<ul>
	<li><a href="https://mila.quebec/en/publications/public-datasets/"> <strong>PUBLIC DATASETS:</strong></a>A collection of datasets collected by MILA lab.</li>
	</ul>
	</li>
	<li> <strong>10 Great Healthcare Data Sets</strong>
	<ul>
	<li><a href="http://www.datasciencecentral.com/profiles/blogs/10-great-healthcare-data-sets"> <strong>10 Great Healthcare Data Sets:</strong></a> Here are 10 great data sets to start playing around with & improve your healthcare data analytics chops.</li>
	</ul>
	</li>
	<li> <strong>The Holy Quran Dataset</strong>
	<ul>
	<li><a href="https://www.kaggle.com/zusmani/the-holy-quran"> <strong>The Holy Quran Dataset:</strong></a> The data contains complete Holy Quran in different 21 languages.</li>
	</ul>
	</li>
	<li> <strong>Victoria's Datasets</strong>
	<ul>
	<li><a href="http://persagen.com/files/ml.html#Datasets"> <strong>Victoria's Datasets Collection</strong></a></li>
	</ul>
	</li>
	<li> <strong>ML-friendly Public Datasets</strong>
	<ul>
	<li><a href="https://www.kaggle.com/annavictoria/ml-friendly-public-datasets/"> <strong>ML-friendly Public Datasets Collection:</strong></a>There are lots of machine learning ready datasets available to use for fun or practice on Kaggle's Public Datasets platform. </li>
	</ul>
	</li>
	<li> <strong>Chest X-ray dataset</strong>
	<ul>
	<li><a href="https://nihcc.app.box.com/v/ChestXray-NIHCC"> <strong>Chest X-ray dataset:</strong></a>The dataset, released by the NIH, contains 112,120 frontal-view X-ray images of 30,805 unique patients, annotated with up to 14 different thoracic pathology labels using NLP methods on radiology reports. </li>
	</ul>
	</li>
	<li> <strong>UT-Austin Computer Vision Group Datasets</strong>
	<ul>
	<li><a href="http://www.cs.utexas.edu/~grauman/research/datasets.html"> <strong>UT-Austin Computer Vision Group Datasets</strong></a></li>
	</ul>
	</li>
	<li> <strong>Lymph Node Detection and Segmentation datasets</strong>
	<ul>
	<li><a href="https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+NodesC"> <strong>Lymph Node Detection and Segmentation datasets:</strong></a>This collection consists of Computed Tomography (CT) images of the mediastinum and abdomen in which lymph node positions are marked by radiologists at the National Institutes of Health, Clinical Center. Radiologists at the Imaging Biomarkers and Computer-Aided Diagnosis Laboratory labeled a total of 388 mediastinal lymph nodes in CT images of 90 patients and a total of 595 abdominal lymph nodes in 86 patients. </li>
	</ul>
	</li>
	<li> <strong>Pancreas Segmentation datasets</strong>
	<ul>
	<li><a href="https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT"> <strong>Pancreas Segmentation datasets:</strong></a>The National Institutes of Health Clinical Center performed 82 abdominal contrast enhanced 3D CT scans (~70 seconds after intravenous contrast injection in portal-venous) from 53 male and 27 female subjects.  Seventeen of the subjects are healthy kidney donors scanned prior to nephrectomy.  The remaining 65 patients were selected by a radiologist from patients who neither had major abdominal pathologies nor pancreatic cancer lesions.  Subjects' ages range from 18 to 76 years with a mean age of 46.8 ± 16.7. The CT scans have resolutions of 512x512 pixels with varying pixel sizes and slice thickness between 1.5 − 2.5 mm, acquired on Philips and Siemens MDCT scanners (120 kVp tube voltage). </li>
	</ul>
	</li>
	<li> <strong>MLDATA repository</strong>
	<ul>
	<li><a href="http://mldata.org/"> <strong>MLDATA repository datasets collection:</strong></a>This web site is as a repository for your machine learning data. </li>
	</ul>
	</li>
	<li> <strong>RENOIR - A Dataset of Real Low-Light Images</strong>
	<ul>
	<li><a href="http://adrianbarburesearch.blogspot.ru/p/renoir-dataset.html"> <strong>RENOIR - A Dataset of Real Low-Light Images:</strong></a>The first publicly available dataset of images corrupted by real low-light noise together with pixel and intensity aligned clean images. It contains about 500 images of 120 scenes that have been collected in low-light setting using three cameras: Cannon T3i, Cannon S90 and a Xiaomi MI3 mobile phone. The dataset is quite large since the images have the original sensor resolution, so each image has about 10 megapixels.</li>
	</ul>
	</li>
	<li> <strong>Pakistan Intellectual Capital dataset</strong>
	<ul>
	<li><a href="https://www.kaggle.com/zusmani/pakistanintellectualcapitalcs"> <strong>Pakistan Intellectual Capital dataset:</strong></a>The dataset contains list of computer science/IT professors from 89 different universities of Pakistan.</li>
	</ul>
	</li>
	<li> <strong>DeepMind Open Source Datasets</strong>
	<ul>
	<li><a href="https://deepmind.com/research/open-source/open-source-datasets/"> <strong>DeepMind Open Source Datasets</strong></a></li>
	</ul>
	</li>
	<li> <strong>AVA Dataset</strong>
	<ul>
	<li><a href="https://research.google.com/ava/"> <strong>AVA Dataset:</strong></a> The AVA dataset densely annotates 80 atomic visual actions in 57.6k movie clips with actions localized in space and time, resulting in 210k action labels with multiple labels per human occurring frequently.</li>
	</ul>
	</li>
	<li> <strong>The NSynth Dataset</strong>
	<ul>
	<li><a href="https://magenta.tensorflow.org/datasets/nsynth"> <strong>The NSynth Dataset:</strong></a> A large-scale and high-quality dataset of annotated musical notes.</li>
	</ul>
	</li>
	<li> <strong>Google Speech Commands Dataset</strong>
	<ul>
	<li><a href="https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html"> <strong>Google Speech Commands Dataset:</strong></a> The dataset has 65,000 one-second long utterances of 30 short words, by thousands of different people, contributed by members of the public through the AIY website.</li>
	</ul>
	</li>
	<li> <strong>70 Amazing Free Data Sources by KDnuggests</strong>
	<ul>
	<li><a href="https://www.kdnuggets.com/2017/12/big-data-free-sources.html"> <strong>70 Amazing Free Data Sources by KDnuggests:</strong></a> 70 free data sources for 2017 on government, crime, health, financial and economic data, marketing and social media, journalism and media, real estate, company directory and review, and more to start working on your data projects.</li>
	</ul>
	</li>
	<li> <strong>List of datasets for machine learning research</strong>
	<ul>
	<li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research"> <strong>List of datasets for machine learning research:</strong></a> This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available.</li>
	</ul>
	</li>
	<li> <strong>data.world</strong>
	<ul>
	<li><a href="https://data.world/"> <strong> data.world datasets collection:</strong></a> data.world is designed for data and the people who work with data. From professional projects to open data, data.world helps you host and share your data, collaborate with your team, and capture context and conclusions as you work.</li>
	</ul>
	</li>
	<li> <strong>Openml data repositories</strong>
	<ul>
	<li><a href="https://www.openml.org/search?type=data"> <strong> Openml datasets collection</strong></a></li>
	</ul>
	</li>
	<li> <strong>25 Datasets for Deep Learning in IoT</strong>
	<ul>
	<li><a href="https://datahub.packtpub.com/deep-learning/25-datasets-deep-learning-iot/"> <strong> 25 Datasets for Deep Learning in IoT</strong></a></li>
	</ul>
	</li>
	<li> <strong>Computer Vision Datasets</strong>
	<ul>
	<li><a href="https://handong1587.github.io/computer_vision/2015/09/24/datasets.html"> <strong> Computer Vision Datasets</strong></a></li>
	</ul>
	</li>
	<li> <strong>deeplearning.net Data collections</strong>
	<ul>
	<li><a href="http://deeplearning.net/datasets/"> <strong> deeplearning.net Data collections</strong></a></li>
	</ul>
	</li>

	</ul>
	
	<p><em><strong>Note:</strong> If you know about any other dataset for triggering data science, machine learning and deep learning research, which is not listed here, please, feel free to comment below with downloadable link. I would like to add it in above list.</em></p>
    <br>
	    <!-- Flag counter -->
	     <img src="http://s05.flagcounter.com/count2/8CG/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"> 
	    
	    <br>
	<!-- Begin MailChimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
	/* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//github.us15.list-manage.com/subscribe/post?u=1c7ce47851d73e8c2a4272286&amp;id=aeafba074a" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2>Subscribe to my mailing list</h2>
<div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
	<label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_1c7ce47851d73e8c2a4272286_aeafba074a" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
	    
	    <div class="post related">

        <a rel="next" id="next-btn" class="btn small square" href="../blog_posts/data_sources.html"> Open Data Resources for Data Science Research - Part-1 →</a>
    </div>

	 </div>


    <footer class="post comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + window.disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</footer>

  </article>


<script type="text/javascript"  
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>  

        <footer>
  <span class="copyright">
    &copy; 2016 Rao Muhammad Umer. All rights reserved. Built with <a href="https://github.com/Kikobeats/uno-zen" target="_blank">Uno Zen</a> theme.
  </span>
</footer>
      </section>
    </main>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
	<script>jQuery = Zepto</script>
    <script src="../assets/js/uno-zen.js?v=d88d979280" type="text/javascript" charset="utf-8"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="javascripts/analytics.js"> </script>
  </body>
</html>
